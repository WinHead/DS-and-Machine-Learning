{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Создание нейронной сети без использования готовых решений\n",
    "\n",
    "__Автор__: Никита Владимирович Блохин (NVBlokhin@fa.ru)\n",
    "\n",
    "Финансовый университет, 2020 г. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "PqC4R7SGseKa"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0J2RM8f5wP33"
   },
   "source": [
    "## 1. Создание нейронов и полносвязных слоев"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2ArJn_nsdZC"
   },
   "source": [
    "1.1. Используя операции над матрицами и векторами из библиотеки `torch`, реализовать нейрон с заданными весами `weights` и `bias`. Прогнать вектор `inputs` через нейрон и вывести результат. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "f4agkY9WqPwe"
   },
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, weights, bias):\n",
    "        # <создать атрибуты объекта weights и bias>\n",
    "    \n",
    "        self.W = weights\n",
    "        self.B = bias\n",
    "    \n",
    "  \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        # forward path implementation\n",
    "        \n",
    "        return torch.matmul(inputs, self.W) + self.B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "HJRkSkHHsb7u"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.8400)\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
    "weights = torch.tensor([-0.2, 0.3, -0.5, 0.7])\n",
    "bias = 3.14\n",
    "\n",
    "# ручная проверка\n",
    "gt = 1.0*(-0.2)+2.0*0.3 + 3.0*(-0.5)+4.0*0.7 + 3.14\n",
    "\n",
    "naive_neuron = Neuron(weights, bias)\n",
    "out = naive_neuron.forward(inputs)\n",
    "print(out)\n",
    "\n",
    "# check\n",
    "assert out == gt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1qJvnwiyty37"
   },
   "source": [
    "1.2 Используя операции над матрицами и векторами из библиотеки `torch`, реализовать полносвязный слой с заданными весами `weights` и `biases`. Прогнать вектор `inputs` через слой и вывести результат. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "fVWF3a9vtx90"
   },
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, weights, biases):\n",
    "    # <создать атрибуты объекта weights и biases>\n",
    "    \n",
    "        self.W = weights\n",
    "        self.B = biases\n",
    "  \n",
    "    def forward(self, inputs):\n",
    "        # forward path implementation\n",
    "        \n",
    "        return torch.matmul(inputs, self.W) + self.B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "Fo-JFnHPuFCS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 4.8400,  0.1700, 10.3900])\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
    "weights = torch.tensor([[-0.2, 0.3, -0.5, 0.7],\n",
    "                        [0.5, -0.91, 0.26, -0.5],\n",
    "                        [-0.26, -0.27, 0.17, 0.87]]).T\n",
    "\n",
    "biases = torch.tensor([3.14, 2.71, 7.2])\n",
    "\n",
    "naive_neuron = Linear(weights, biases)\n",
    "out = naive_neuron.forward(inputs)\n",
    "print(out)\n",
    "\n",
    "# check\n",
    "assert out.size()[0] == 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mQtsJzcxuyGd"
   },
   "source": [
    "1.3 Реализовать полносвязный слой из __2.1.2__ таким образом, чтобы он мог принимать на вход матрицу (батч) с данными. Продемонстрировать работу.\n",
    "Результатом прогона сквозь слой должна быть матрица размера `batch_size` x `n_neurons`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "Z8IizmtsuhO1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.7900,  0.9200,  9.0850],\n",
      "        [ 6.1400, -2.1000,  6.9000],\n",
      "        [ 2.0400,  0.7610,  6.7260]])\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.tensor([[1, 2, 3, 2.5], \n",
    "                       [2, 5, -1, 2], \n",
    "                       [-1.5, 2.7, 3.3, -0.8]])\n",
    "batch_size = 3\n",
    "n_neurons = 3\n",
    "\n",
    "out = naive_neuron.forward(inputs)\n",
    "print(out)\n",
    "\n",
    "# check\n",
    "assert out.size()[0] == batch_size and out.size()[1] == n_neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OQ2OxH4_vBLu"
   },
   "source": [
    "1.4 Используя операции над матрицами и векторами из библиотеки `torch`, реализовать полносвязный слой из `n_neurons` нейронов с `n_features` весами у каждого нейрона (инициализируются из стандартного нормального распределения). Прогнать вектор `inputs` через слой и вывести результат. Результатом прогона сквозь слой должна быть матрица размера `batch_size` x `n_neurons`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1705,  0.0199,  0.8357],\n",
       "        [-0.6815,  1.2642, -1.0900]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.empty(2, 3).normal_(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "IOv52EdovASs"
   },
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, n_features, n_neurons):\n",
    "    # <создать атрибуты объекта weights и biases>\n",
    "    \n",
    "        # Создаем случайные веса и смещение нужных размерностей\n",
    "        self.W = torch.empty(n_neurons, n_features).normal_(0,1)\n",
    "        self.B = torch.empty(n_neurons).normal_(0,1)    \n",
    "  \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        # На этот раз веса необходимо протранспонировать. X * W.T + B\n",
    "        return torch.matmul(inputs, self.W.T) + self.B\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5754, -1.1635,  4.7333],\n",
      "        [-2.9272, -2.6839,  7.4405],\n",
      "        [ 1.5523, -0.3188,  2.4747],\n",
      "        [-7.1090, -5.6461, 12.3475]])\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.tensor([[1, 2], \n",
    "                       [2, 5], \n",
    "                       [-1.5, 2.7],\n",
    "                       [3, 12]])\n",
    "n_features = inputs.size()[1]\n",
    "n_neurons = 3\n",
    "batch_size = inputs.size()[0]\n",
    "\n",
    "linear_layer = Linear(n_features, n_neurons)\n",
    "out = linear_layer.forward(inputs)\n",
    "print(out)\n",
    "\n",
    "# check\n",
    "assert out.size()[0] == batch_size and out.size()[1] == n_neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IPG4UqL4wajI"
   },
   "source": [
    "1.5 Используя решение из __1.4__, создать 2 полносвязных слоя и пропустить матрицу `inputs` последовательно через эти два слоя. Количество нейронов в первом слое выбрать произвольно, количество нейронов во втором слое выбрать так, чтобы результатом прогона являлась матрица (3x7). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "RjjQIQlTxJE6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ -6.5179,  -4.4551,  -3.0809,  -4.3016,  -1.5625,   1.8166,   6.7143],\n",
      "        [-22.2727,  -5.6442,  -5.4176, -18.6326, -15.6484,   3.3593,  19.6188],\n",
      "        [-11.9712,  -1.1063,  -4.1587,   1.3513,   4.9698,  -3.7623,  -5.2983]])\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.tensor([[1, 2, 3, 2.5], \n",
    "                       [2, 5, -1, 2], \n",
    "                       [-1.5, 2.7, 3.3, -0.8]])\n",
    "\n",
    "n_features = inputs.size()[1]\n",
    "n_neurons = 5\n",
    "batch_size = inputs.size()[0]\n",
    "\n",
    "linear_layer_1 = Linear(n_features, n_neurons)\n",
    "linear_layer_2 = Linear(n_neurons, 7)\n",
    "\n",
    "out = linear_layer_2.forward(linear_layer_1.forward(inputs))\n",
    "print(out)\n",
    "\n",
    "# check\n",
    "assert out.size()[0] == 3 and out.size()[1] == 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для красоты обернем в класс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NastiaNN:\n",
    "    \n",
    "    def __init__(self, n_layers, n_in_futures= 4, n_out_futures= 7):\n",
    "        \n",
    "        # Инициализируем слои нашей сетки\n",
    "        self.layers = []\n",
    "        for i in range(n_layers):\n",
    "            if i == n_layers - 1:\n",
    "                self.layers.append(Linear(n_neurons, n_out_futures))\n",
    "                print(f\"Создан FC слой формой ({n_in_futures} на {n_out_futures})\")\n",
    "                break\n",
    "                \n",
    "            n_neurons = random.randint(2,10)\n",
    "            self.layers.append(Linear(n_in_futures, n_neurons))\n",
    "            print(f\"Создан FC слой формой ({n_in_futures} на {n_neurons})\")\n",
    "            n_in_futures = n_neurons\n",
    "            \n",
    "            \n",
    "    def forward(self, X):\n",
    "        out = X\n",
    "        for layer in self.layers:\n",
    "            out = layer.forward(out)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Создан FC слой формой (4 на 9)\n",
      "Создан FC слой формой (9 на 5)\n",
      "Создан FC слой формой (5 на 3)\n",
      "Создан FC слой формой (3 на 6)\n",
      "Создан FC слой формой (6 на 7)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[  37.1062,  -47.2284,   83.9885,   -3.5957, -244.5765,   76.6474,\n",
       "          -38.6390],\n",
       "        [  -4.6869,  -17.3361,   23.9658,  -40.2316,  -52.6167,   18.6769,\n",
       "          -42.1530],\n",
       "        [  44.9802,  -37.4012,   70.4583,   24.1208, -221.5627,   65.3560,\n",
       "          -10.7416]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network = NastiaNN(5)\n",
    "\n",
    "network.forward(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Па бам!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cRVH_2K7xTBC"
   },
   "source": [
    "## 2. Создание функций активации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B9kngE6Fxs9D"
   },
   "source": [
    "2.1 Используя операции над матрицами и векторами из библиотеки `torch`, реализовать функцию активации ReLU:\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/f4353f4e3e484130504049599d2e7b040793e1eb)\n",
    "\n",
    "Создать матрицу размера (4,3), заполненную числами из стандартного нормального распределения, и проверить работоспособность функции активации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "jZLvMRByxSTC"
   },
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def forward(self, inputs):\n",
    "        # <реализовать логику ReLU>\n",
    "        inputs[inputs < 0] = 0\n",
    "    \n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Начальный тензор: \n",
      "tensor([[ 0.4534, -0.3830,  2.4398],\n",
      "        [ 1.2556, -0.0780,  1.7658],\n",
      "        [-1.0187,  1.1666, -0.6434],\n",
      "        [ 0.4732, -0.9432, -0.8744]])\n",
      "\n",
      "Тензор после ReLu: \n",
      "tensor([[0.4534, 0.0000, 2.4398],\n",
      "        [1.2556, 0.0000, 1.7658],\n",
      "        [0.0000, 1.1666, 0.0000],\n",
      "        [0.4732, 0.0000, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.randn((4,3))\n",
    "print(f'Начальный тензор: \\n{inputs}')\n",
    "relu = ReLU()\n",
    "print(f'\\nТензор после ReLu: \\n{relu.forward(inputs)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "puExCWiKyTtb"
   },
   "source": [
    "2.2 Используя операции над матрицами и векторами из библиотеки `torch`, реализовать функцию активации softmax:\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/6d7500d980c313da83e4117da701bf7c8f1982f5)\n",
    "\n",
    "Создать матрицу размера (4,3), заполненную числами из стандартного нормального распределения, и проверить работоспособность функции активации. Строки матрицы трактовать как выходы линейного слоя некоторого классификатора для 4 различных примеров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0389],\n",
       "        [1.6593],\n",
       "        [2.5922],\n",
       "        [4.8061]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.reshape(torch.exp(inputs).sum(1), (-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "fXNcFlqqyKHl"
   },
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def forward(self, inputs):\n",
    "    # <реализовать логику Softmax>\n",
    "    \n",
    "        exp_inputs = torch.exp(inputs)\n",
    "    \n",
    "        return exp_inputs / torch.reshape(exp_inputs.sum(1), (-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Начальный тензор: \n",
      "tensor([[ 1.0842, -1.3692, -1.5656],\n",
      "        [-0.6316,  1.3125,  1.2832],\n",
      "        [ 0.1270, -4.9263, -2.1252],\n",
      "        [ 1.5804,  0.9235, -1.5799]])\n",
      "\n",
      "Тензор после Softmax: \n",
      "tensor([[0.8646, 0.0744, 0.0611],\n",
      "        [0.0677, 0.4730, 0.4593],\n",
      "        [0.8996, 0.0057, 0.0946],\n",
      "        [0.6407, 0.3322, 0.0272]])\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.randn((4,3))\n",
    "print(f'Начальный тензор: \\n{inputs}')\n",
    "sm = Softmax()\n",
    "print(f'\\nТензор после Softmax: \\n{sm.forward(inputs)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vxVK2TYez_Ye"
   },
   "source": [
    "2.3 Используя операции над матрицами и векторами из библиотеки `torch`, реализовать функцию активации ELU:\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/eb23becd37c3602c4838e53f532163279192e4fd)\n",
    "\n",
    "Создать матрицу размера (4,3), заполненную числами из стандартного нормального распределения, и проверить работоспособность функции активации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "NzMz7HDLySxK"
   },
   "outputs": [],
   "source": [
    "class ELU:\n",
    "    def __init__(self, alpha):\n",
    "        # <создать атрибут объекта alpha>\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # <реализовать логику ReLU>\n",
    "        \n",
    "        alpha_tensor = self.alpha * (torch.exp(inputs) - 1)\n",
    "        return torch.where(inputs < 0, inputs, alpha_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Начальный тензор: \n",
      "tensor([[-0.0215, -0.0943, -1.7014],\n",
      "        [-1.9720,  0.0455,  0.8348],\n",
      "        [ 0.7892, -1.1035, -0.7040],\n",
      "        [ 0.5905,  1.6785, -1.3647]])\n",
      "\n",
      "Тензор после elu: \n",
      "tensor([[-0.0215, -0.0943, -1.7014],\n",
      "        [-1.9720,  0.0233,  0.6522],\n",
      "        [ 0.6009, -1.1035, -0.7040],\n",
      "        [ 0.4025,  2.1787, -1.3647]])\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.randn((4,3))\n",
    "print(f'Начальный тензор: \\n{inputs}')\n",
    "elu = ELU(0.5)\n",
    "print(f'\\nТензор после elu: \\n{elu.forward(inputs)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0peh8r-20Pof"
   },
   "source": [
    "## 3. Создание функции потерь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EY-k3eEs0f7f"
   },
   "source": [
    "3.1 Используя операции над матрицами и векторами из библиотеки `torch`, реализовать функцию потерь MSE:\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/e258221518869aa1c6561bb75b99476c4734108e)\n",
    "\n",
    "Создать полносвязный слой с 1 нейроном, прогнать через него батч `inputs` и посчитать значение MSE, трактуя вектор `y` как вектор правильных ответов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "f9-wdj5Tz-br"
   },
   "outputs": [],
   "source": [
    "class MSELoss:\n",
    "    def forward(self, y_pred, y_true):\n",
    "        \n",
    "        return ((y_true - y_pred) ** 2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "NAyuDU9F1Vuz"
   },
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[1, 2, 3, 2.5], \n",
    "                       [2, 5, -1, 2], \n",
    "                       [-1.5, 2.7, 3.3, -0.8]])\n",
    "\n",
    "y = torch.tensor([2, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE loss: 39.20469284057617\n"
     ]
    }
   ],
   "source": [
    "# Создаем Fully connected слой с одним нероном\n",
    "linear_layer = Linear(4, 1)\n",
    "\n",
    "mse_loss = MSELoss()\n",
    "print(f'MSE loss: {mse_loss.forward(linear_layer.forward(inputs), y)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uaR7rILd1eWR"
   },
   "source": [
    "3.2 Используя операции над матрицами и векторами из библиотеки `torch`, реализовать функцию потерь Categorical Cross-Entropy:\n",
    "\n",
    "<img src=\"https://i.ibb.co/93gy1dN/Screenshot-9.png\" width=\"200\">\n",
    "\n",
    "Создать полносвязный слой с 3 нейронами и прогнать через него батч `inputs`. Полученный результат пропустить через функцию активации softmax. Посчитать значение CCE, трактуя вектор `y` как вектор правильных ответов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "hQl8pJsT3HcF"
   },
   "outputs": [],
   "source": [
    "class CategoricalCrossentropyLoss:\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # <реализовать логику CCE>\n",
    "        \n",
    "        # dim = 1, так как имеем дело с несколькими батчами\n",
    "        return -1 * (y_true * torch.log(y_pred)).sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "s7Qoupfo1ZGJ"
   },
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[1, 2, 3, 2.5], \n",
    "                        [2, 5, -1, 2], \n",
    "                        [-1.5, 2.7, 3.3, -0.8]])\n",
    "y = torch.tensor([1, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CCE loss: tensor([10.0317,  0.3843,  7.0138])\n"
     ]
    }
   ],
   "source": [
    "# Создаем Fully connected слой с тремя неронами\n",
    "linear_layer = Linear(4, 3)\n",
    "\n",
    "softmax = Softmax()\n",
    "cce_loss = CategoricalCrossentropyLoss()\n",
    "\n",
    "print(f'CCE loss: {cce_loss.forward(softmax.forward(linear_layer.forward(inputs)), y)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fA6dbanf44_4"
   },
   "source": [
    "3.3 Модифицировать 2.3.1, добавив L2-регуляризацию.\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/d92ca2429275bfdc0474523babbafe014ca8b580)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "ADsZxD-h4_Os"
   },
   "outputs": [],
   "source": [
    "class MSELossL:\n",
    "    \n",
    "    def __init__(self, lambda_, layer):\n",
    "        # <создать атрибут объекта alpha>\n",
    "        self.lambda_ = lambda_\n",
    "        self.layer = layer\n",
    "\n",
    "    def data_loss(self, y_pred, y_true):\n",
    "        # <подсчет первого слагаемого из формулы>\n",
    "        return ((y_true - y_pred) ** 2).sum()\n",
    "    \n",
    "    def reg_loss(self, layer):\n",
    "        # используйте атрибуты объекта layer, в которых хранятся веса слоя\n",
    "        # <подсчет второго слагаемого из формулы>\n",
    "        return self.lambda_ * (layer.W ** 2).sum()\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        return self.data_loss(y_pred, y_true) + self.reg_loss(self.layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE loss: 18.79985237121582\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.tensor([[1, 2, 3, 2.5], \n",
    "                        [2, 5, -1, 2], \n",
    "                        [-1.5, 2.7, 3.3, -0.8]])\n",
    "y = torch.tensor([1, 0, 0])\n",
    "\n",
    "# Создаем Fully connected слой с тремя неронами\n",
    "linear_layer = Linear(4, 3)\n",
    "layer_out = linear_layer.forward(inputs)\n",
    "\n",
    "softmax = Softmax()\n",
    "mse_loss = MSELossL(1.5, linear_layer)\n",
    "\n",
    "print(f'MSE loss: {mse_loss.forward(softmax.forward(layer_out), y)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w049ZSdR6qQi"
   },
   "source": [
    "## 4. Обратное распространение ошибки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pBtCfSME9W7Q"
   },
   "source": [
    "4.1 Используя один нейрон и SGD (1 пример за шаг), решите задачу регрессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "id": "4xmI-QJ66WAF"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X, y, coef = make_regression(n_features=4, n_informative=4, coef=True, bias=0.5)\n",
    "X = torch.from_numpy(X).type(torch.float32)# <преобразуйте массивы numpy в тензоры torch с типом torch.float32\n",
    "y = torch.from_numpy(y).type(torch.float32)# <преобразуйте массивы numpy в тензоры torch с типом torch.float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KpPSPYSpD9Ey"
   },
   "source": [
    "[Граф вычислений для этой задачи](https://i.ibb.co/2dhDxZx/photo-2021-02-15-17-18-04.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fc1sXtGd_J-y"
   },
   "source": [
    "4.1.1 Модифицируйте класс `MSELoss` из __2.3.1__, реализовав расчет производной относительно предыдущего слоя\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "llFigkqd_JRU"
   },
   "outputs": [],
   "source": [
    "class MSELoss:\n",
    "    def forward(self, y_pred, y_true):\n",
    "        return ((y_true - y_pred) ** 2).mean()\n",
    "\n",
    "    def backward(self, y_pred, y_true):\n",
    "        # df/dc = 2(с-y) - из графа для вычислений\n",
    "        self.dinput = 2 * (y_pred - y_true)# df/dc\n",
    "        return self.dinput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GY7ForfM97UQ"
   },
   "source": [
    "4.1.2. Модифицируйте класс `Neuron` из __2.1.1__:\n",
    "\n",
    "  1) Сделайте так, чтобы веса нейрона инициализировались из стандартного нормального распределения\n",
    "\n",
    "  2) Реализуйте расчет градиента относительно весов `weights` и `bias`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "id": "L0KqxPJU9kAN"
   },
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, n_inputs):\n",
    "        # <создать атрибуты объекта weights и bias>\n",
    "        # Создаем случайные веса и смещение нужных размерностей\n",
    "        self.W = torch.randn(n_inputs)\n",
    "        self.B = torch.randn(1)    \n",
    "  \n",
    "    def forward(self, inputs):\n",
    "        return torch.matmul(inputs, self.W.T) + self.B\n",
    "      \n",
    "    def backward(self, dvalue):\n",
    "        # dvalue - значение производной, которое приходит нейрону от следующего слоя сети\n",
    "        # в данном случае это будет значение df/dc (созданное методом backwards у объекта MSELoss)\n",
    "        self.dweights = dvalue # df/dW - из графа вычислений = 2e*1 = dvalue\n",
    "        self.dinput =  dvalue * self.W # df/wX = 2e*w = dvalue * W  ----- Зачем здесь dinput?\n",
    "        self.dbias = dvalue# df/db = 2e = dvalue\n",
    "        \n",
    "        # Возвращаем градиент весов и смещения\n",
    "        return self.dweights, self.dbias\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.3527])\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
    "\n",
    "naive_neuron = Neuron(4)\n",
    "out = naive_neuron.forward(inputs)\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rKcO4zOLACxM"
   },
   "source": [
    "4.1.3 Допишите цикл для настройки весов нейрона\n",
    "\n",
    "[SGD](https://ru.wikipedia.org/wiki/%D0%A1%D1%82%D0%BE%D1%85%D0%B0%D1%81%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D0%B9_%D0%B3%D1%80%D0%B0%D0%B4%D0%B8%D0%B5%D0%BD%D1%82%D0%BD%D1%8B%D0%B9_%D1%81%D0%BF%D1%83%D1%81%D0%BA)\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/dda3670f8a8996a0d3bf80856bb4a166cc8db6d4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 4])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "id": "_g_FvwvmALJd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Average loss= 11027.037109375\n",
      "Epoch 1, Average loss= 10654.146484375\n",
      "Epoch 2, Average loss= 10362.1376953125\n",
      "Epoch 3, Average loss= 10132.431640625\n",
      "Epoch 4, Average loss= 9951.0478515625\n",
      "Epoch 5, Average loss= 9807.369140625\n",
      "Epoch 6, Average loss= 9693.259765625\n",
      "Epoch 7, Average loss= 9602.4404296875\n",
      "Epoch 8, Average loss= 9530.0322265625\n",
      "Epoch 9, Average loss= 9472.21875\n",
      "Epoch 10, Average loss= 9426.00390625\n",
      "Epoch 11, Average loss= 9389.02734375\n",
      "Epoch 12, Average loss= 9359.4189453125\n",
      "Epoch 13, Average loss= 9335.6962890625\n",
      "Epoch 14, Average loss= 9316.6796875\n",
      "Epoch 15, Average loss= 9301.4287109375\n",
      "Epoch 16, Average loss= 9289.1953125\n",
      "Epoch 17, Average loss= 9279.3779296875\n",
      "Epoch 18, Average loss= 9271.5009765625\n",
      "Epoch 19, Average loss= 9265.17578125\n",
      "Epoch 20, Average loss= 9260.099609375\n",
      "Epoch 21, Average loss= 9256.0224609375\n",
      "Epoch 22, Average loss= 9252.75\n",
      "Epoch 23, Average loss= 9250.1220703125\n",
      "Epoch 24, Average loss= 9248.009765625\n",
      "Epoch 25, Average loss= 9246.314453125\n",
      "Epoch 26, Average loss= 9244.9521484375\n",
      "Epoch 27, Average loss= 9243.8583984375\n",
      "Epoch 28, Average loss= 9242.98046875\n",
      "Epoch 29, Average loss= 9242.2734375\n",
      "Epoch 30, Average loss= 9241.7060546875\n",
      "Epoch 31, Average loss= 9241.2529296875\n",
      "Epoch 32, Average loss= 9240.88671875\n",
      "Epoch 33, Average loss= 9240.5927734375\n",
      "Epoch 34, Average loss= 9240.3564453125\n",
      "Epoch 35, Average loss= 9240.16796875\n",
      "Epoch 36, Average loss= 9240.015625\n",
      "Epoch 37, Average loss= 9239.89453125\n",
      "Epoch 38, Average loss= 9239.7958984375\n",
      "Epoch 39, Average loss= 9239.71875\n",
      "Epoch 40, Average loss= 9239.654296875\n",
      "Epoch 41, Average loss= 9239.603515625\n",
      "Epoch 42, Average loss= 9239.5625\n",
      "Epoch 43, Average loss= 9239.529296875\n",
      "Epoch 44, Average loss= 9239.5009765625\n",
      "Epoch 45, Average loss= 9239.48046875\n",
      "Epoch 46, Average loss= 9239.462890625\n",
      "Epoch 47, Average loss= 9239.44921875\n",
      "Epoch 48, Average loss= 9239.4384765625\n",
      "Epoch 49, Average loss= 9239.4287109375\n",
      "Epoch 50, Average loss= 9239.421875\n",
      "Epoch 51, Average loss= 9239.416015625\n",
      "Epoch 52, Average loss= 9239.4111328125\n",
      "Epoch 53, Average loss= 9239.408203125\n",
      "Epoch 54, Average loss= 9239.4052734375\n",
      "Epoch 55, Average loss= 9239.40234375\n",
      "Epoch 56, Average loss= 9239.400390625\n",
      "Epoch 57, Average loss= 9239.3984375\n",
      "Epoch 58, Average loss= 9239.396484375\n",
      "Epoch 59, Average loss= 9239.3955078125\n",
      "Epoch 60, Average loss= 9239.39453125\n",
      "Epoch 61, Average loss= 9239.3935546875\n",
      "Epoch 62, Average loss= 9239.3916015625\n",
      "Epoch 63, Average loss= 9239.3916015625\n",
      "Epoch 64, Average loss= 9239.390625\n",
      "Epoch 65, Average loss= 9239.390625\n",
      "Epoch 66, Average loss= 9239.390625\n",
      "Epoch 67, Average loss= 9239.3896484375\n",
      "Epoch 68, Average loss= 9239.3896484375\n",
      "Epoch 69, Average loss= 9239.3896484375\n",
      "Epoch 70, Average loss= 9239.3896484375\n",
      "Epoch 71, Average loss= 9239.388671875\n",
      "Epoch 72, Average loss= 9239.388671875\n",
      "Epoch 73, Average loss= 9239.3876953125\n",
      "Epoch 74, Average loss= 9239.3876953125\n",
      "Epoch 75, Average loss= 9239.3876953125\n",
      "Epoch 76, Average loss= 9239.3876953125\n",
      "Epoch 77, Average loss= 9239.3876953125\n",
      "Epoch 78, Average loss= 9239.3876953125\n",
      "Epoch 79, Average loss= 9239.3876953125\n",
      "Epoch 80, Average loss= 9239.3876953125\n",
      "Epoch 81, Average loss= 9239.3876953125\n",
      "Epoch 82, Average loss= 9239.3876953125\n",
      "Epoch 83, Average loss= 9239.388671875\n",
      "Epoch 84, Average loss= 9239.388671875\n",
      "Epoch 85, Average loss= 9239.388671875\n",
      "Epoch 86, Average loss= 9239.388671875\n",
      "Epoch 87, Average loss= 9239.388671875\n",
      "Epoch 88, Average loss= 9239.388671875\n",
      "Epoch 89, Average loss= 9239.388671875\n",
      "Epoch 90, Average loss= 9239.388671875\n",
      "Epoch 91, Average loss= 9239.388671875\n",
      "Epoch 92, Average loss= 9239.388671875\n",
      "Epoch 93, Average loss= 9239.388671875\n",
      "Epoch 94, Average loss= 9239.388671875\n",
      "Epoch 95, Average loss= 9239.388671875\n",
      "Epoch 96, Average loss= 9239.388671875\n",
      "Epoch 97, Average loss= 9239.388671875\n",
      "Epoch 98, Average loss= 9239.388671875\n",
      "Epoch 99, Average loss= 9239.388671875\n"
     ]
    }
   ],
   "source": [
    "n_inputs = X.size(1)# <размерность элемента выборки >\n",
    "learning_rate = 0.001 #  скорость обучения\n",
    "n_epoch = 100 #  количество эпох\n",
    "\n",
    "neuron = Neuron(n_inputs)\n",
    "loss = MSELoss()\n",
    "\n",
    "losses = []\n",
    "for epoch in range(100):\n",
    "    mean_epoch_loss = []\n",
    "    # 100 раз выбираем пару X b y\n",
    "    for x_example, y_example in zip(X, y):\n",
    "        # forward pass\n",
    "        y_pred = neuron.forward(x_example)# <прогон через нейрон>\n",
    "        curr_loss = loss.forward(y_pred, y_example)# <прогон через функцию потерь>\n",
    "        losses.append(curr_loss)\n",
    "        mean_epoch_loss.append(curr_loss)\n",
    "        \n",
    "        # backprop\n",
    "        # <вызов методов backward>\n",
    "        # обратите внимание на последовательность вызовов: от конца к началу\n",
    "        \n",
    "        # Получаем градиенты весов и смещений\n",
    "        dW, dB = neuron.backward( loss.backward(y_pred, y_example) )\n",
    "        \n",
    "        # <шаг оптимизации для весов (weights и bias) нейрона>\n",
    "        neuron.W -= learning_rate * dW\n",
    "        neuron.B -= learning_rate * dB\n",
    "        \n",
    "    print(f\"Epoch {epoch}, Average loss= {np.array(mean_epoch_loss).mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Поигравшись с lr становится понятно, что нейрон просто обчень быстро скатывается в локальный минимум.\n",
    "# В данном случае, после 37 похи loss перестает уменьшаться"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ebibge9VEgF7"
   },
   "source": [
    "4.2 Решите задачу 2.4.1, используя пакетный градиентный спуск"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "as-QeWSdOELd"
   },
   "source": [
    "Вычисления для этой задачи: \n",
    "[1](https://i.ibb.co/rmtQT6P/photo-2021-02-15-18-00-43.jpg)\n",
    "[2](https://i.ibb.co/NmCFVnQ/photo-2021-02-15-18-01-17.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Average loss= 11029.3935546875\n",
      "Epoch 1, Average loss= 11029.3935546875\n",
      "Epoch 2, Average loss= 11029.3935546875\n",
      "Epoch 3, Average loss= 11029.3935546875\n",
      "Epoch 4, Average loss= 11029.3935546875\n",
      "Epoch 5, Average loss= 11029.3935546875\n",
      "Epoch 6, Average loss= 11029.3935546875\n",
      "Epoch 7, Average loss= 11029.3935546875\n",
      "Epoch 8, Average loss= 11029.3935546875\n",
      "Epoch 9, Average loss= 11029.3935546875\n",
      "Epoch 10, Average loss= 11029.3935546875\n",
      "Epoch 11, Average loss= 11029.3935546875\n",
      "Epoch 12, Average loss= 11029.3935546875\n",
      "Epoch 13, Average loss= 11029.3935546875\n",
      "Epoch 14, Average loss= 11029.3935546875\n",
      "Epoch 15, Average loss= 11029.3935546875\n",
      "Epoch 16, Average loss= 11029.3935546875\n",
      "Epoch 17, Average loss= 11029.3935546875\n",
      "Epoch 18, Average loss= 11029.3935546875\n",
      "Epoch 19, Average loss= 11029.3935546875\n",
      "Epoch 20, Average loss= 11029.3935546875\n",
      "Epoch 21, Average loss= 11029.3935546875\n",
      "Epoch 22, Average loss= 11029.3935546875\n",
      "Epoch 23, Average loss= 11029.3935546875\n",
      "Epoch 24, Average loss= 11029.3935546875\n",
      "Epoch 25, Average loss= 11029.3935546875\n",
      "Epoch 26, Average loss= 11029.3935546875\n",
      "Epoch 27, Average loss= 11029.3935546875\n",
      "Epoch 28, Average loss= 11029.3935546875\n",
      "Epoch 29, Average loss= 11029.3935546875\n",
      "Epoch 30, Average loss= 11029.3935546875\n",
      "Epoch 31, Average loss= 11029.3935546875\n",
      "Epoch 32, Average loss= 11029.3935546875\n",
      "Epoch 33, Average loss= 11029.3935546875\n",
      "Epoch 34, Average loss= 11029.3935546875\n",
      "Epoch 35, Average loss= 11029.3935546875\n",
      "Epoch 36, Average loss= 11029.3935546875\n",
      "Epoch 37, Average loss= 11029.3935546875\n",
      "Epoch 38, Average loss= 11029.3935546875\n",
      "Epoch 39, Average loss= 11029.3935546875\n",
      "Epoch 40, Average loss= 11029.3935546875\n",
      "Epoch 41, Average loss= 11029.3935546875\n",
      "Epoch 42, Average loss= 11029.3935546875\n",
      "Epoch 43, Average loss= 11029.3935546875\n",
      "Epoch 44, Average loss= 11029.3935546875\n",
      "Epoch 45, Average loss= 11029.3935546875\n",
      "Epoch 46, Average loss= 11029.3935546875\n",
      "Epoch 47, Average loss= 11029.3935546875\n",
      "Epoch 48, Average loss= 11029.3935546875\n",
      "Epoch 49, Average loss= 11029.3935546875\n",
      "Epoch 50, Average loss= 11029.3935546875\n",
      "Epoch 51, Average loss= 11029.3935546875\n",
      "Epoch 52, Average loss= 11029.3935546875\n",
      "Epoch 53, Average loss= 11029.3935546875\n",
      "Epoch 54, Average loss= 11029.3935546875\n",
      "Epoch 55, Average loss= 11029.3935546875\n",
      "Epoch 56, Average loss= 11029.3935546875\n",
      "Epoch 57, Average loss= 11029.3935546875\n",
      "Epoch 58, Average loss= 11029.3935546875\n",
      "Epoch 59, Average loss= 11029.3935546875\n",
      "Epoch 60, Average loss= 11029.3935546875\n",
      "Epoch 61, Average loss= 11029.3935546875\n",
      "Epoch 62, Average loss= 11029.3935546875\n",
      "Epoch 63, Average loss= 11029.3935546875\n",
      "Epoch 64, Average loss= 11029.3935546875\n",
      "Epoch 65, Average loss= 11029.3935546875\n",
      "Epoch 66, Average loss= 11029.3935546875\n",
      "Epoch 67, Average loss= 11029.3935546875\n",
      "Epoch 68, Average loss= 11029.3935546875\n",
      "Epoch 69, Average loss= 11029.3935546875\n",
      "Epoch 70, Average loss= 11029.3935546875\n",
      "Epoch 71, Average loss= 11029.3935546875\n",
      "Epoch 72, Average loss= 11029.3935546875\n",
      "Epoch 73, Average loss= 11029.3935546875\n",
      "Epoch 74, Average loss= 11029.3935546875\n",
      "Epoch 75, Average loss= 11029.3935546875\n",
      "Epoch 76, Average loss= 11029.3935546875\n",
      "Epoch 77, Average loss= 11029.3935546875\n",
      "Epoch 78, Average loss= 11029.3935546875\n",
      "Epoch 79, Average loss= 11029.3935546875\n",
      "Epoch 80, Average loss= 11029.3935546875\n",
      "Epoch 81, Average loss= 11029.3935546875\n",
      "Epoch 82, Average loss= 11029.3935546875\n",
      "Epoch 83, Average loss= 11029.3935546875\n",
      "Epoch 84, Average loss= 11029.3935546875\n",
      "Epoch 85, Average loss= 11029.3935546875\n",
      "Epoch 86, Average loss= 11029.3935546875\n",
      "Epoch 87, Average loss= 11029.3935546875\n",
      "Epoch 88, Average loss= 11029.3935546875\n",
      "Epoch 89, Average loss= 11029.3935546875\n",
      "Epoch 90, Average loss= 11029.3935546875\n",
      "Epoch 91, Average loss= 11029.3935546875\n",
      "Epoch 92, Average loss= 11029.3935546875\n",
      "Epoch 93, Average loss= 11029.3935546875\n",
      "Epoch 94, Average loss= 11029.3935546875\n",
      "Epoch 95, Average loss= 11029.3935546875\n",
      "Epoch 96, Average loss= 11029.3935546875\n",
      "Epoch 97, Average loss= 11029.3935546875\n",
      "Epoch 98, Average loss= 11029.3935546875\n",
      "Epoch 99, Average loss= 11029.3935546875\n"
     ]
    }
   ],
   "source": [
    "n_inputs = X.size(1)# <размерность элемента выборки >\n",
    "learning_rate = 0.1 #  скорость обучения\n",
    "n_epoch = 100 #  количество эпох\n",
    "\n",
    "neuron = Neuron(n_inputs)\n",
    "loss = MSELoss()\n",
    "optimizer = torch.optim.SGD([neuron.W, neuron.B], lr=learning_rate)\n",
    "\n",
    "losses = []\n",
    "for epoch in range(100):\n",
    "    mean_epoch_loss = []\n",
    "    # 100 раз выбираем пару X b y\n",
    "    for x_example, y_example in zip(X, y):\n",
    "        # forward pass\n",
    "        y_pred = neuron.forward(x_example)# <прогон через нейрон>\n",
    "        curr_loss = loss.forward(y_pred, y_example)# <прогон через функцию потерь>\n",
    "        losses.append(curr_loss)\n",
    "        mean_epoch_loss.append(curr_loss)\n",
    "        \n",
    "        loss.backward(y_pred, y_example)          \n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "    print(f\"Epoch {epoch}, Average loss= {np.array(mean_epoch_loss).mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dr9qq4H_J3zt"
   },
   "source": [
    "4.2.1 Модифицируйте класс `MSELoss` из __3.1__, реализовав расчет производной относительно предыдущего слоя с учетом того, что теперь работа ведется с батчами, а не с индивидуальными примерами\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L8wjk9iPMQ4x"
   },
   "outputs": [],
   "source": [
    "class MSELoss:\n",
    "  def forward(self, y_pred, y_true):\n",
    "    return # <реализовать логику MSE>\n",
    "\n",
    "  def backward(self, y_pred, y_true):\n",
    "    self.dinput = # df/dy^\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E3fSHCEtJjX8"
   },
   "source": [
    "4.2.2. Модифицируйте класс `Neuron` из __4.1.2__:\n",
    "\n",
    "  1) Реализуйте метод `forward` таким образом, чтобы он мог принимать на вход матрицу (батч) с данными. \n",
    "\n",
    "  2) Реализуйте расчет градиента относительно весов `weights` и `bias` с учетом того, что теперь работа ведется с батчами, а не с индивидуальными примерами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o_OpuAP0Jpz1"
   },
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "  def __init__(self, n_inputs):\n",
    "    # <создать атрибуты объекта weights и bias>\n",
    "    pass\n",
    "  \n",
    "  def forward(self, inputs):\n",
    "    return # <реализовать логику нейрона>\n",
    "  \n",
    "  def backward(self, dvalue):\n",
    "    # dvalue - значение градиента, которое приходит нейрону от следующего слоя сети\n",
    "    # в данном случае это будет градиент L по y^ (созданный методом backwards у объекта MSELoss)\n",
    "    self.dweights = # df/dW\n",
    "    self.dbias = # df/db\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zO-NZrgKMBFx"
   },
   "source": [
    "4.2.3 Допишите цикл для настройки весов нейрона"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zqwm_7eqJim1"
   },
   "outputs": [],
   "source": [
    "n_inputs = # <размерность элемента выборки >\n",
    "learning_rate = 0.1 #  скорость обучения\n",
    "n_epoch = 100 #  количество эпох\n",
    "\n",
    "neuron = Neuron(n_inputs)\n",
    "loss = MSELoss()\n",
    "\n",
    "\n",
    "for epoch in range(100):\n",
    "    # forward pass\n",
    "    y_pred = # <прогон через нейрон>\n",
    "    curr_loss = # <прогон через функцию потерь>\n",
    "    losses.append(curr_loss)\n",
    "\n",
    "    # backprop\n",
    "    # <вызов методов backward>\n",
    "    # обратите внимание на последовательность вызовов: от конца к началу\n",
    "\n",
    "    # <шаг оптимизации для весов (weights и bias) нейрона>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "16VtP159OdMk"
   },
   "source": [
    "4.3  Используя один полносвязный слой и  пакетный градиетный спуск, решите задачу регрессии из __2.4.1__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uj5febreSSZ7"
   },
   "source": [
    "4.3.1 Модифицируйте класс `Linear` из __1.4__. ([вычисление градиентов](https://i.ibb.co/kgVR6m6/photo-2021-02-15-21-30-28.jpg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9zWuhaLdSB2_"
   },
   "outputs": [],
   "source": [
    "class Linear:\n",
    "  def __init__(self, n_features, n_neurons):\n",
    "    # <создать атрибуты объекта weights и biases>\n",
    "    pass\n",
    "  \n",
    "  def forward(self, inputs):\n",
    "    return # <реализовать логику слоя>\n",
    "\n",
    "  def backward(self, dvalues):\n",
    "    self.dweights = # df/dW\n",
    "    self.dbiases = # df/db\n",
    "    self.dinputs = # df/dX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j3w1hT9MS_Lt"
   },
   "source": [
    "4.3.2 Создайте слой с одним нейроном. Используя класс MSELoss из 2.4.2, убедитесь, что модель обучается"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RTkJV-F8TVuN"
   },
   "source": [
    "4.4 Используя наработки из 2.4, создайте нейросеть и решите задачу регрессии.\n",
    "\n",
    "Предлагаемая архитектура: \n",
    "1. Полносвязный слой с 10 нейронами\n",
    "2. Активация ReLU\n",
    "3. Полносвязный слой с 1 нейроном"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "axUjpPz-SvS1"
   },
   "outputs": [],
   "source": [
    "X = torch.linspace(-1, 1, 100).view(-1, 1)\n",
    "y = X.pow(2) + 0.2 * torch.rand(X.size()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LXoiNxkpTziV"
   },
   "outputs": [],
   "source": [
    "class Activation_ReLU:\n",
    "  def forward(self, inputs):\n",
    "    self.inputs = inputs\n",
    "    self.output = inputs.clip(min=0)\n",
    "    return self.output\n",
    "  \n",
    "  def backward(self, dvalues):\n",
    "    self.dinputs = dvalues.clone()\n",
    "    self.dinputs[self.inputs <= 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tXhspwW6T44T"
   },
   "outputs": [],
   "source": [
    "# создание компонентов сети\n",
    "# fc1 = \n",
    "# relu1 = \n",
    "# fc2 = \n",
    "\n",
    "loss = MSELoss()\n",
    "lr = 0.02\n",
    "\n",
    "ys = []\n",
    "for epoch in range(2001):\n",
    "  # <forward pass>\n",
    "  # fc1 > relu1 > fc2 > loss\n",
    "\n",
    "  data_loss = # <прогон через функцию потерь>\n",
    "\n",
    "  if epoch % 200 == 0:\n",
    "    print(f'epoch {epoch} mean loss {data_loss}')\n",
    "    ys.append(out)\n",
    "  \n",
    "  # <backprop> \n",
    "  # loss > fc2 > relu1 > fc1\n",
    "\n",
    "  # <шаг оптимизации для fc1>\n",
    "\n",
    "  # <шаг оптимизации для fc2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kpKi0OfoUkwk"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(len(ys), 1, figsize=(10, 40))\n",
    "for ax, y_ in zip(axs, ys):\n",
    "  ax.scatter(X.numpy(), y.numpy(), color = \"orange\")\n",
    "  ax.plot(X.numpy(), y_.numpy(), 'g-', lw=3)\n",
    "  ax.set_xlim(-1.05, 1.5)\n",
    "  ax.set_ylim(-0.25, 1.25)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPDgJRHjuyArfKO8ZT68MsS",
   "name": "02_NN_blocks_backprop_v1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
